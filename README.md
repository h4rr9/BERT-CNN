# CNN on BERT Embeddings

Testing the performance of CNN and pretrained BERT embeddings on the GLUE Tasks

# BERT Model

The BERT model used is the BERT-Base, Uncased: 12-layer, 768-hidden, 12-heads, 110M parameters

the tokenization.py file is based on the google-research/bert repo https://github.com/google-research/bert
